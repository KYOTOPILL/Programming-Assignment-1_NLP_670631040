{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10004592,"sourceType":"datasetVersion","datasetId":6158355}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:22:32.681669Z","iopub.execute_input":"2024-11-29T16:22:32.682043Z","iopub.status.idle":"2024-11-29T16:22:33.678006Z","shell.execute_reply.started":"2024-11-29T16:22:32.682004Z","shell.execute_reply":"2024-11-29T16:22:33.676720Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/alice29-text-dataset/alice29.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"with open('/kaggle/input/alice29-text-dataset/alice29.txt', 'r', encoding='utf-8') as file:\n    text = file.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:22:33.680265Z","iopub.execute_input":"2024-11-29T16:22:33.680844Z","iopub.status.idle":"2024-11-29T16:22:33.689124Z","shell.execute_reply.started":"2024-11-29T16:22:33.680797Z","shell.execute_reply":"2024-11-29T16:22:33.687941Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# NLTK","metadata":{}},{"cell_type":"code","source":"# Install nltk\n!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:22:33.690461Z","iopub.execute_input":"2024-11-29T16:22:33.690862Z","iopub.status.idle":"2024-11-29T16:22:44.887662Z","shell.execute_reply.started":"2024-11-29T16:22:33.690816Z","shell.execute_reply":"2024-11-29T16:22:44.886275Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import nltk\n# Download required NLTK resources for tokenization and stopword handling\nnltk.download('punkt_tab')  # Punkt tokenizer for splitting sentences\nnltk.download('stopwords')  # Stopwords list for filtering common words\n\n# Import libraries for tokenization, stopword handling, frequency analysis, and text processing\nfrom nltk.tokenize import word_tokenize, sent_tokenize  # Functions for tokenizing words and sentences\nfrom nltk.corpus import stopwords  # Stopwords to filter out common words\nfrom collections import Counter  # To count word frequencies\nimport re  # Regular expressions for text cleaning\nimport time  # To measure execution time\n\n# Start the timer to measure runtime\nstart_time = time.time()\n\n# Function to clean the text data\ndef clean_text(text):\n    text = text.lower()  # Convert all text to lowercase \n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation marks\n    text = re.sub(r'\\d+', '', text)  # Remove numerical digits\n    return text\n\ncleaned_text = clean_text(text)  # Apply the function to the input text\n\n# Save the cleaned text to a file\noutput_cleaned_file = '/kaggle/working/cleaned_nltk.txt'  # File path for saving the cleaned text\nwith open(output_cleaned_file, 'w') as f:\n    f.write(cleaned_text)  # Write the cleaned text to the file\n\n# Tokenize the text into sentences\nsentences = sent_tokenize(text)  # Split the text into individual sentences\n\n# Save tokenized sentences to a file\noutput_words_sentences_file = '/kaggle/working/sentences_nltk.txt'  # File path for saving tokenized sentences\nwith open(output_words_sentences_file, 'w') as f:\n    f.write(\"Tokenized Sentences:\\n\") \n    for sentence in sentences:\n        f.write(sentence + \"\\n\")  # Write each sentence to a new line\n\n# Tokenize the cleaned text into words\nwords = word_tokenize(cleaned_text)  # Split the cleaned text into individual words\n\n# Remove stopwords from the tokenized words\nstop_words = set(stopwords.words('english'))  # Load the list of stopwords\nfiltered_words = [word for word in words if word not in stop_words]  # Filter out stopwords\n\n# Save the tokenized and filtered words to a file\noutput_words_file = '/kaggle/working/words_nltk.txt'  # File path for saving tokenized words\nwith open(output_words_file, 'w') as f:\n    f.write('\\n'.join(filtered_words))  # Write each word to a new line\n\n# Perform frequency analysis to find the top 10 most common words\nword_freq = Counter(filtered_words)  # Count occurrences of each word\ntop_10_words = word_freq.most_common(10)  # Get the top 10 most frequent words\n\n# Convert the top 10 words and their frequencies to a DataFrame\ntop_words_df = pd.DataFrame(top_10_words, columns=[\"Word\", \"Frequency\"])\n\n# Save the DataFrame as a CSV file for tabular output\noutput_top_words_file = '/kaggle/working/top10words_nltk.csv'  # File path for saving the top 10 words\ntop_words_df.to_csv(output_top_words_file, index=False)  # Save DataFrame to a CSV file without the index\n\n# Measure the total runtime for the process\nend_time = time.time()  # Record the end time\nnltk_runtime = end_time - start_time  # Calculate the elapsed time\n\n# Save the runtime to a file\noutput_runtime_file = '/kaggle/working/time_compares_nltk.txt'  # File path for saving runtime data\nwith open(output_runtime_file, 'w') as f:\n    f.write(f\"NLTK Runtime: {nltk_runtime:.4f} seconds\\n\")  # Write the runtime to the file\n\n# Print output details for verification\nprint(\"Cleaned text saved to 'cleaned_nltk.txt'\")  \nprint(\"Tokenized sentences saved to 'sentences_nltk.txt'\")  \nprint(\"Tokenized words saved to 'words_nltk.txt'\")  \nprint(\"Top 10 words saved to 'top10words_nltk.txt'\") \nprint(\"Runtime saved to 'time_compares_nltk.txt'\") \nprint(f\"NLTK Runtime: {nltk_runtime:.4f} seconds\")  # Print the total runtime\nprint(\"Top 10 most common words:\")  # Print the header for the top 10 words\n# Print the DataFrame as a table in the console\nprint(top_words_df)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:22:44.890445Z","iopub.execute_input":"2024-11-29T16:22:44.890840Z","iopub.status.idle":"2024-11-29T16:22:46.498707Z","shell.execute_reply.started":"2024-11-29T16:22:44.890803Z","shell.execute_reply":"2024-11-29T16:22:46.497539Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nCleaned text saved to 'cleaned_nltk.txt'\nTokenized sentences saved to 'sentences_nltk.txt'\nTokenized words saved to 'words_nltk.txt'\nTop 10 words saved to 'top10words_nltk.txt'\nRuntime saved to 'time_compares_nltk.txt'\nNLTK Runtime: 0.1698 seconds\nTop 10 most common words:\n      Word  Frequency\n0     said        462\n1    alice        385\n2   little        128\n3      one        101\n4     know         86\n5     like         85\n6    would         83\n7     went         83\n8    could         77\n9  thought         74\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# textBlob","metadata":{}},{"cell_type":"code","source":"# Install TextBlob\n!pip install textblob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:22:46.500226Z","iopub.execute_input":"2024-11-29T16:22:46.500640Z","iopub.status.idle":"2024-11-29T16:22:58.239476Z","shell.execute_reply.started":"2024-11-29T16:22:46.500595Z","shell.execute_reply":"2024-11-29T16:22:58.238092Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: textblob in /opt/conda/lib/python3.10/site-packages (0.18.0.post0)\nCollecting nltk>=3.8 (from textblob)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.66.4)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.9.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Import necessary libraries\nfrom textblob import TextBlob  # For text processing, tokenization, and analysis\nfrom collections import Counter  # To perform word frequency analysis\nimport re  # For text cleaning using regular expressions\nimport time  # To measure execution time\n\n# Start the timer to measure runtime\nstart_time = time.time()\n\n# Clean the text data\ndef clean_text(text):\n    text = text.lower()  # Convert the text to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation marks\n    text = re.sub(r'\\d+', '', text)  # Remove numeric digits\n    return text\n\ncleaned_text = clean_text(text)  # Apply the function to the input text\n\n# Save the cleaned text to a file\noutput_cleaned_file = '/kaggle/working/cleaned_textBlob.txt'  # File path to save the cleaned text\nwith open(output_cleaned_file, 'w') as f:\n    f.write(cleaned_text)  # Write the cleaned text to the file\n\n# Tokenize sentences and words using TextBlob\nblob = TextBlob(text)  # Create a TextBlob object from the input text\n\n# Sentence tokenization\nsentences = blob.sentences  # Tokenize the text into sentences\n\n# Save tokenized sentences to a file\noutput_words_sentences_file = '/kaggle/working/sentences_textBlob.txt'  # File path for saving tokenized sentences\nwith open(output_words_sentences_file, 'w') as f:\n    f.write(\"Tokenized Sentences:\\n\")  \n    for sentence in sentences:\n        f.write(str(sentence) + \"\\n\")  # Write each sentence on a new line\n\n# Tokenize the cleaned text into words\nblob = TextBlob(cleaned_text)  # Create a TextBlob object from the cleaned text\nwords = blob.words  # Extract words from the cleaned text\n\n# Remove stopwords manually\nstop_words = set([\"the\", \"and\", \"is\", \"in\", \"to\", \"of\", \"a\", \"that\", \"it\", \"on\", \"with\", \"for\", \"as\", \"this\", \"was\", \"by\"])  # Define a custom list of stopwords\nfiltered_words = [word for word in words if word not in stop_words]  # Remove stopwords from the tokenized words\n\n# Save tokenized words to a file\noutput_words_file = '/kaggle/working/words_textBlob.txt'  # File path for saving tokenized words\nwith open(output_words_file, 'w') as f:\n    f.write('\\n'.join(filtered_words))  # Write each word to a new line\n\n# Perform frequency analysis for the top 10 words\nword_freq = Counter(filtered_words)  # Count occurrences of each word\ntop_10_words = word_freq.most_common(10)  # Extract the top 10 most frequent words\n\n# Convert the top 10 words and their frequencies to a DataFrame\ntop_words_df = pd.DataFrame(top_10_words, columns=[\"Word\", \"Frequency\"])\n\n# Save the DataFrame as a CSV file for tabular output\noutput_top_words_file = '/kaggle/working/top10words_textBlob.csv'  # File path for saving the top 10 words\ntop_words_df.to_csv(output_top_words_file, index=False)  # Save DataFrame to a CSV file without the index\n\n# Stop the timer to measure runtime\nend_time = time.time()  # Record the end time\ntextblob_runtime = end_time - start_time  # Calculate the total runtime\n\n# Save runtime to a file\noutput_runtime_file = '/kaggle/working/time_compares_textBlob.txt'  # File path for saving the runtime\nwith open(output_runtime_file, 'w') as f:\n    f.write(f\"TextBlob Runtime: {textblob_runtime:.4f} seconds\\n\")  # Write the runtime in seconds\n\n# Print output details for verification\nprint(\"Cleaned text saved to 'cleaned_textBlob.txt'\")  \nprint(\"Tokenized words saved to 'words_textBlob.txt'\")  \nprint(\"Tokenized sentences and words saved to 'words_sentences_textBlob.txt'\")  \nprint(\"Top 10 words saved to 'top10words_textBlob.txt'\")  \nprint(\"Runtime saved to 'time_compares_textBlob.txt'\") \nprint(f\"TextBlob Runtime: {textblob_runtime:.4f} seconds\")  # Print the total runtime\nprint(\"Top 10 most common words:\")  # Print the header for the top 10 words\n# Print the DataFrame as a table in the console\nprint(top_words_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:22:58.241134Z","iopub.execute_input":"2024-11-29T16:22:58.241528Z","iopub.status.idle":"2024-11-29T16:22:58.653094Z","shell.execute_reply.started":"2024-11-29T16:22:58.241492Z","shell.execute_reply":"2024-11-29T16:22:58.651977Z"}},"outputs":[{"name":"stdout","text":"Cleaned text saved to 'cleaned_textBlob.txt'\nTokenized words saved to 'words_textBlob.txt'\nTokenized sentences and words saved to 'words_sentences_textBlob.txt'\nTop 10 words saved to 'top10words_textBlob.txt'\nRuntime saved to 'time_compares_textBlob.txt'\nTextBlob Runtime: 0.3492 seconds\nTop 10 most common words:\n    Word  Frequency\n0    she        536\n1   said        462\n2      i        401\n3  alice        385\n4    you        362\n5    her        247\n6     at        209\n7    all        180\n8    had        178\n9    but        166\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# spaCy","metadata":{}},{"cell_type":"code","source":"# Install spaCy and download the English model\n!pip install spacy\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:22:58.654313Z","iopub.execute_input":"2024-11-29T16:22:58.654604Z","iopub.status.idle":"2024-11-29T16:23:29.578525Z","shell.execute_reply.started":"2024-11-29T16:22:58.654576Z","shell.execute_reply":"2024-11-29T16:23:29.577370Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.8.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.4.0,>=8.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.3.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.12.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.9.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (70.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.4.1)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\nRequirement already satisfied: blis<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\nCollecting numpy>=1.19.0 (from spacy)\n  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 2.0.2 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\ncatboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.0.2 which is incompatible.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\nibis-framework 7.1.0 requires numpy<2,>=1, but you have numpy 2.0.2 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 17.0.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmatplotlib 3.7.5 requires numpy<2,>=1.20, but you have numpy 2.0.2 which is incompatible.\ntensorflow 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\ntensorflow-transform 0.14.0 requires numpy<2,>=1.16, but you have numpy 2.0.2 which is incompatible.\nxarray 2024.9.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.0.2\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Import necessary libraries\nimport spacy  # spaCy for advanced NLP tasks including tokenization\nfrom collections import Counter  # To perform word frequency analysis\nimport re  # For text cleaning using regular expressions\nimport time  # To measure execution time\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_sm\")  # Load the English language model for spaCy\n\n# Clean the text data\ndef clean_text(text):\n    text = text.lower()  # Convert the text to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation marks\n    text = re.sub(r'\\d+', '', text)  # Remove numerical digits\n    return text\n\ncleaned_text = clean_text(text)  # Apply the cleaning function to the input text\n\n# Save the cleaned text to a file\noutput_cleaned_file = '/kaggle/working/cleaned_spacy.txt'  # File path for saving the cleaned text\nwith open(output_cleaned_file, 'w') as f:\n    f.write(cleaned_text)  # Write the cleaned text to the file\n\n# Start the timer to measure runtime for spaCy\nstart_time = time.time()\n\n# Tokenize sentences using spaCy\ndoc = nlp(text)  # Process the input text with spaCy\n\n# Sentence tokenization\nsentences = [sent.text for sent in doc.sents]  # Extract sentences from the processed text\n\n# Save tokenized sentences to a file\noutput_words_sentences_file = '/kaggle/working/sentences_spacy.txt'  # File path for saving tokenized sentences\nwith open(output_words_sentences_file, 'w') as f:\n    f.write(\"Tokenized Sentences:\\n\") \n    for sentence in sentences:\n        f.write(sentence + \"\\n\")  # Write each sentence on a new line\n\n# Tokenize the cleaned text into words\ndoc = nlp(cleaned_text)  # Process the cleaned text with spaCy\nwords = [token.text for token in doc if token.is_alpha]  # Extract only alphabetic tokens (ignore numbers and symbols)\n\n# Save tokenized words to a file\noutput_words_file = '/kaggle/working/words_spacy.txt'  # File path for saving tokenized words\nwith open(output_words_file, 'w') as f:\n    f.write('\\n'.join(words))  # Write each word to a new line\n\n# Perform frequency analysis for the top 10 words\nword_freq = Counter(words)  # Count occurrences of each word\ntop_10_words = word_freq.most_common(10)  # Extract the top 10 most frequent words\n\n# Convert the top 10 words and their frequencies to a DataFrame\ntop_words_df = pd.DataFrame(top_10_words, columns=[\"Word\", \"Frequency\"])\n\n# Save the DataFrame as a CSV file for tabular output\noutput_top_words_file = '/kaggle/working/top10words_spaCy.csv'  # File path for saving the top 10 words\ntop_words_df.to_csv(output_top_words_file, index=False)  # Save DataFrame to a CSV file without the index\n\n# Stop the timer to measure runtime\nend_time = time.time()  # Record the end time\nspacy_runtime = end_time - start_time  # Calculate the total runtime\n\n# Save runtime to a file\noutput_runtime_file = '/kaggle/working/time_compares_spacy.txt'  # File path for saving the runtime\nwith open(output_runtime_file, 'w') as f:\n    f.write(f\"spaCy Runtime: {spacy_runtime:.4f} seconds\\n\")  # Write the runtime in seconds\n\n# Print output details for verification\nprint(\"Step 1: Cleaned text saved to 'cleaned_spacy.txt'\") \nprint(\"Step 2: Tokenized words saved to 'words_spacy.txt'\")  \nprint(\"Step 3: Tokenized sentences and words saved to 'words_sentences_spacy.txt'\")  \nprint(\"Step 4: Top 10 words saved to 'top10words_spacy.txt'\")  \nprint(\"Step 5: Runtime saved to 'time_compares_spacy.txt'\")  \nprint(f\"spaCy Runtime: {spacy_runtime:.4f} seconds\")  # Print the total runtime\nprint(\"Top 10 most common words:\")  # Print the header for the top 10 words\n# Print the DataFrame as a table in the console\nprint(top_words_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:23:29.580905Z","iopub.execute_input":"2024-11-29T16:23:29.581419Z","iopub.status.idle":"2024-11-29T16:23:41.800880Z","shell.execute_reply.started":"2024-11-29T16:23:29.581368Z","shell.execute_reply":"2024-11-29T16:23:41.799738Z"}},"outputs":[{"name":"stdout","text":"Step 1: Cleaned text saved to 'cleaned_spacy.txt'\nStep 2: Tokenized words saved to 'words_spacy.txt'\nStep 3: Tokenized sentences and words saved to 'words_sentences_spacy.txt'\nStep 4: Top 10 words saved to 'top10words_spacy.txt'\nStep 5: Runtime saved to 'time_compares_spacy.txt'\nspaCy Runtime: 8.6274 seconds\nTop 10 most common words:\n   Word  Frequency\n0   the       1630\n1   and        844\n2    to        721\n3     a        627\n4   she        543\n5    it        534\n6    of        507\n7     i        503\n8  said        462\n9   you        407\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Compare Runtime","metadata":{}},{"cell_type":"code","source":"import pandas as pd  # Import pandas to create a comparison table\n\n# Example runtime data for NLTK, TextBlob, and spaCy (replace with actual values)\nruntime_data = {\n    \"Framework\": [\"NLTK\", \"TextBlob\", \"spaCy\"],\n    \"Runtime (seconds)\": [0.1513, 0.4294, 8.5534]  # Replace these values with actual runtimes\n}\n\n# Convert the data to a pandas DataFrame\nruntime_comparison_df = pd.DataFrame(runtime_data)\n\n# Save the runtime comparison table as a CSV file\noutput_runtime_comparison_file = \"/kaggle/working/runtime_comparison.csv\"\nruntime_comparison_df.to_csv(output_runtime_comparison_file, index=False)\n\n# Display the DataFrame for user reference\nruntime_comparison_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:23:41.802134Z","iopub.execute_input":"2024-11-29T16:23:41.802505Z","iopub.status.idle":"2024-11-29T16:23:41.818603Z","shell.execute_reply.started":"2024-11-29T16:23:41.802471Z","shell.execute_reply":"2024-11-29T16:23:41.817510Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  Framework  Runtime (seconds)\n0      NLTK             0.1513\n1  TextBlob             0.4294\n2     spaCy             8.5534","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Framework</th>\n      <th>Runtime (seconds)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NLTK</td>\n      <td>0.1513</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TextBlob</td>\n      <td>0.4294</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spaCy</td>\n      <td>8.5534</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9}]}